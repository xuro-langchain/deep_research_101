{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "899fe885",
   "metadata": {},
   "source": [
    "# Optimized Researcher\n",
    "\n",
    "This notebook demonstrates the final deep research iteration that creates comprehensive reports using a supervisor architecture. The system:\n",
    "\n",
    "\n",
    "1. Uses a **supervisor agent** with specialized nodes for planning out the report\n",
    "2. Can spin up ad-hoc **researcher subagents** to gather information as required for the report\n",
    "3. Produces a well-structured report with introduction, researched body sections, and conclusion\n",
    "\n",
    "## Setup\n",
    "\n",
    "We'll start by changing our directory and importing our environment variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78b47a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/robertxu/Desktop/Projects/eng/open_deep_research/src\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4f95b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124355db",
   "metadata": {},
   "source": [
    "Next, we'll import all the helpers we need for the agent's implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d78104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.optimized.configuration import (\n",
    "    Configuration,\n",
    ")\n",
    "from agents.optimized.prompts import (\n",
    "    clarify_with_user_instructions,\n",
    "    compress_research_simple_human_message,\n",
    "    compress_research_system_prompt,\n",
    "    final_report_generation_prompt,\n",
    "    lead_researcher_prompt,\n",
    "    research_system_prompt,\n",
    "    transform_messages_into_research_topic_prompt,\n",
    ")\n",
    "from agents.optimized.state import (\n",
    "    AgentInputState,\n",
    "    AgentState,\n",
    "    ClarifyWithUser,\n",
    "    ConductResearch,\n",
    "    ResearchComplete,\n",
    "    ResearcherOutputState,\n",
    "    ResearcherState,\n",
    "    ResearchQuestion,\n",
    "    SupervisorState,\n",
    ")\n",
    "from agents.optimized.utils import (\n",
    "    anthropic_websearch_called,\n",
    "    get_all_tools,\n",
    "    get_api_key_for_model,\n",
    "    get_model_token_limit,\n",
    "    get_notes_from_tool_calls,\n",
    "    get_today_str,\n",
    "    is_token_limit_exceeded,\n",
    "    openai_websearch_called,\n",
    "    remove_up_to_last_ai_message,\n",
    "    think_tool,\n",
    ")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "# Initialize a configurable model that we will use throughout the agent\n",
    "configurable_model = init_chat_model(\n",
    "    configurable_fields=(\"model\", \"max_tokens\", \"api_key\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d52fdb",
   "metadata": {},
   "source": [
    "## Pre-Research Workflow\n",
    "\n",
    "We'll start by implementing some steps we want our agent to handle before conducting any research.\n",
    "\n",
    "### Nodes\n",
    "\n",
    "The following nodes allow our agent to ask any clarifying questions to the user before creating a research brief to guide our supervisor agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac0e12e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    "    filter_messages,\n",
    "    get_buffer_string,\n",
    ")\n",
    "\n",
    "from langgraph.types import Command, Literal\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "async def clarify_with_user(state: AgentState, config: RunnableConfig) -> Command[Literal[\"write_research_brief\", \"__end__\"]]:\n",
    "    \"\"\"Analyze user messages and ask clarifying questions if the research scope is unclear.\n",
    "    \n",
    "    This function determines whether the user's request needs clarification before proceeding\n",
    "    with research. If clarification is disabled or not needed, it proceeds directly to research.\n",
    "    \n",
    "    Args:\n",
    "        state: Current agent state containing user messages\n",
    "        config: Runtime configuration with model settings and preferences\n",
    "        \n",
    "    Returns:\n",
    "        Command to either end with a clarifying question or proceed to research brief\n",
    "    \"\"\"\n",
    "    # Step 1: Check if clarification is enabled in configuration\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    if not configurable.allow_clarification:\n",
    "        # Skip clarification step and proceed directly to research\n",
    "        return Command(goto=\"write_research_brief\")\n",
    "    \n",
    "    # Step 2: Prepare the model for structured clarification analysis\n",
    "    messages = state[\"messages\"]\n",
    "    model_config = {\n",
    "        \"model\": configurable.research_model,\n",
    "        \"max_tokens\": configurable.research_model_max_tokens,\n",
    "        \"api_key\": get_api_key_for_model(configurable.research_model, config),\n",
    "        \"tags\": [\"langsmith:nostream\"]\n",
    "    }\n",
    "    \n",
    "    # Configure model with structured output and retry logic\n",
    "    clarification_model = (\n",
    "        configurable_model\n",
    "        .with_structured_output(ClarifyWithUser)\n",
    "        .with_retry(stop_after_attempt=configurable.max_structured_output_retries)\n",
    "        .with_config(model_config)\n",
    "    )\n",
    "    \n",
    "    # Step 3: Analyze whether clarification is needed\n",
    "    prompt_content = clarify_with_user_instructions.format(\n",
    "        messages=get_buffer_string(messages), \n",
    "        date=get_today_str()\n",
    "    )\n",
    "    response = await clarification_model.ainvoke([HumanMessage(content=prompt_content)])\n",
    "    \n",
    "    # Step 4: Route based on clarification analysis\n",
    "    if response.need_clarification:\n",
    "        # End with clarifying question for user\n",
    "        return Command(\n",
    "            goto=END, \n",
    "            update={\"messages\": [AIMessage(content=response.question)]}\n",
    "        )\n",
    "    else:\n",
    "        # Proceed to research with verification message\n",
    "        return Command(\n",
    "            goto=\"write_research_brief\", \n",
    "            update={\"messages\": [AIMessage(content=response.verification)]}\n",
    "        )\n",
    "\n",
    "\n",
    "async def write_research_brief(state: AgentState, config: RunnableConfig) -> Command[Literal[\"research_supervisor\"]]:\n",
    "    \"\"\"Transform user messages into a structured research brief and initialize supervisor.\n",
    "    \n",
    "    This function analyzes the user's messages and generates a focused research brief\n",
    "    that will guide the research supervisor. It also sets up the initial supervisor\n",
    "    context with appropriate prompts and instructions.\n",
    "    \n",
    "    Args:\n",
    "        state: Current agent state containing user messages\n",
    "        config: Runtime configuration with model settings\n",
    "        \n",
    "    Returns:\n",
    "        Command to proceed to research supervisor with initialized context\n",
    "    \"\"\"\n",
    "    # Step 1: Set up the research model for structured output\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    research_model_config = {\n",
    "        \"model\": configurable.research_model,\n",
    "        \"max_tokens\": configurable.research_model_max_tokens,\n",
    "        \"api_key\": get_api_key_for_model(configurable.research_model, config),\n",
    "        \"tags\": [\"langsmith:nostream\"]\n",
    "    }\n",
    "    \n",
    "    # Configure model for structured research question generation\n",
    "    research_model = (\n",
    "        configurable_model\n",
    "        .with_structured_output(ResearchQuestion)\n",
    "        .with_retry(stop_after_attempt=configurable.max_structured_output_retries)\n",
    "        .with_config(research_model_config)\n",
    "    )\n",
    "    \n",
    "    # Step 2: Generate structured research brief from user messages\n",
    "    prompt_content = transform_messages_into_research_topic_prompt.format(\n",
    "        messages=get_buffer_string(state.get(\"messages\", [])),\n",
    "        date=get_today_str()\n",
    "    )\n",
    "    response = await research_model.ainvoke([HumanMessage(content=prompt_content)])\n",
    "    \n",
    "    # Step 3: Initialize supervisor with research brief and instructions\n",
    "    supervisor_system_prompt = lead_researcher_prompt.format(\n",
    "        date=get_today_str(),\n",
    "        max_concurrent_research_units=configurable.max_concurrent_research_units,\n",
    "        max_researcher_iterations=configurable.max_researcher_iterations\n",
    "    )\n",
    "    \n",
    "    return Command(\n",
    "        goto=\"research_supervisor\", \n",
    "        update={\n",
    "            \"research_brief\": response.research_brief,\n",
    "            \"supervisor_messages\": {\n",
    "                \"type\": \"override\",\n",
    "                \"value\": [\n",
    "                    SystemMessage(content=supervisor_system_prompt),\n",
    "                    HumanMessage(content=response.research_brief)\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cd1901",
   "metadata": {},
   "source": [
    "## Researcher Subagent\n",
    "\n",
    "We'll implement our individual researchers next. Each individual agent will be a ReAct agent capable of using search tools.\n",
    "\n",
    "### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86afd90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio \n",
    "\n",
    "async def researcher(state: ResearcherState, config: RunnableConfig) -> Command[Literal[\"researcher_tools\"]]:\n",
    "    \"\"\"Individual researcher that conducts focused research on specific topics.\n",
    "    \n",
    "    This researcher is given a specific research topic by the supervisor and uses\n",
    "    available tools (search, think_tool, MCP tools) to gather comprehensive information.\n",
    "    It can use think_tool for strategic planning between searches.\n",
    "    \n",
    "    Args:\n",
    "        state: Current researcher state with messages and topic context\n",
    "        config: Runtime configuration with model settings and tool availability\n",
    "        \n",
    "    Returns:\n",
    "        Command to proceed to researcher_tools for tool execution\n",
    "    \"\"\"\n",
    "    # Step 1: Load configuration and validate tool availability\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    researcher_messages = state.get(\"researcher_messages\", [])\n",
    "    \n",
    "    # Get all available research tools (search, MCP, think_tool)\n",
    "    tools = await get_all_tools(config)\n",
    "    if len(tools) == 0:\n",
    "        raise ValueError(\n",
    "            \"No tools found to conduct research: Please configure either your \"\n",
    "            \"search API or add MCP tools to your configuration.\"\n",
    "        )\n",
    "    \n",
    "    # Step 2: Configure the researcher model with tools\n",
    "    research_model_config = {\n",
    "        \"model\": configurable.research_model,\n",
    "        \"max_tokens\": configurable.research_model_max_tokens,\n",
    "        \"api_key\": get_api_key_for_model(configurable.research_model, config),\n",
    "        \"tags\": [\"langsmith:nostream\"]\n",
    "    }\n",
    "    \n",
    "    # Prepare system prompt with MCP context if available\n",
    "    researcher_prompt = research_system_prompt.format(\n",
    "        mcp_prompt=configurable.mcp_prompt or \"\", \n",
    "        date=get_today_str()\n",
    "    )\n",
    "    \n",
    "    # Configure model with tools, retry logic, and settings\n",
    "    research_model = (\n",
    "        configurable_model\n",
    "        .bind_tools(tools)\n",
    "        .with_retry(stop_after_attempt=configurable.max_structured_output_retries)\n",
    "        .with_config(research_model_config)\n",
    "    )\n",
    "    \n",
    "    # Step 3: Generate researcher response with system context\n",
    "    messages = [SystemMessage(content=researcher_prompt)] + researcher_messages\n",
    "    response = await research_model.ainvoke(messages)\n",
    "    \n",
    "    # Step 4: Update state and proceed to tool execution\n",
    "    return Command(\n",
    "        goto=\"researcher_tools\",\n",
    "        update={\n",
    "            \"researcher_messages\": [response],\n",
    "            \"tool_call_iterations\": state.get(\"tool_call_iterations\", 0) + 1\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Tool Execution Helper Function\n",
    "async def execute_tool_safely(tool, args, config):\n",
    "    \"\"\"Safely execute a tool with error handling.\"\"\"\n",
    "    try:\n",
    "        return await tool.ainvoke(args, config)\n",
    "    except Exception as e:\n",
    "        return f\"Error executing tool: {str(e)}\"\n",
    "\n",
    "\n",
    "async def researcher_tools(state: ResearcherState, config: RunnableConfig) -> Command[Literal[\"researcher\", \"compress_research\"]]:\n",
    "    \"\"\"Execute tools called by the researcher, including search tools and strategic thinking.\n",
    "    \n",
    "    This function handles various types of researcher tool calls:\n",
    "    1. think_tool - Strategic reflection that continues the research conversation\n",
    "    2. Search tools (tavily_search, web_search) - Information gathering\n",
    "    3. MCP tools - External tool integrations\n",
    "    4. ResearchComplete - Signals completion of individual research task\n",
    "    \n",
    "    Args:\n",
    "        state: Current researcher state with messages and iteration count\n",
    "        config: Runtime configuration with research limits and tool settings\n",
    "        \n",
    "    Returns:\n",
    "        Command to either continue research loop or proceed to compression\n",
    "    \"\"\"\n",
    "    # Step 1: Extract current state and check early exit conditions\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    researcher_messages = state.get(\"researcher_messages\", [])\n",
    "    most_recent_message = researcher_messages[-1]\n",
    "    \n",
    "    # Early exit if no tool calls were made (including native web search)\n",
    "    has_tool_calls = bool(most_recent_message.tool_calls)\n",
    "    has_native_search = (\n",
    "        openai_websearch_called(most_recent_message) or \n",
    "        anthropic_websearch_called(most_recent_message)\n",
    "    )\n",
    "    \n",
    "    if not has_tool_calls and not has_native_search:\n",
    "        return Command(goto=\"compress_research\")\n",
    "    \n",
    "    # Step 2: Handle other tool calls (search, MCP tools, etc.)\n",
    "    tools = await get_all_tools(config)\n",
    "    tools_by_name = {\n",
    "        tool.name if hasattr(tool, \"name\") else tool.get(\"name\", \"web_search\"): tool \n",
    "        for tool in tools\n",
    "    }\n",
    "    \n",
    "    # Execute all tool calls in parallel\n",
    "    tool_calls = most_recent_message.tool_calls\n",
    "    tool_execution_tasks = [\n",
    "        execute_tool_safely(tools_by_name[tool_call[\"name\"]], tool_call[\"args\"], config) \n",
    "        for tool_call in tool_calls\n",
    "    ]\n",
    "    observations = await asyncio.gather(*tool_execution_tasks)\n",
    "    \n",
    "    # Create tool messages from execution results\n",
    "    tool_outputs = [\n",
    "        ToolMessage(\n",
    "            content=observation,\n",
    "            name=tool_call[\"name\"],\n",
    "            tool_call_id=tool_call[\"id\"]\n",
    "        ) \n",
    "        for observation, tool_call in zip(observations, tool_calls)\n",
    "    ]\n",
    "    \n",
    "    # Step 3: Check late exit conditions (after processing tools)\n",
    "    exceeded_iterations = state.get(\"tool_call_iterations\", 0) >= configurable.max_react_tool_calls\n",
    "    research_complete_called = any(\n",
    "        tool_call[\"name\"] == \"ResearchComplete\" \n",
    "        for tool_call in most_recent_message.tool_calls\n",
    "    )\n",
    "    \n",
    "    if exceeded_iterations or research_complete_called:\n",
    "        # End research and proceed to compression\n",
    "        return Command(\n",
    "            goto=\"compress_research\",\n",
    "            update={\"researcher_messages\": tool_outputs}\n",
    "        )\n",
    "    \n",
    "    # Continue research loop with tool results\n",
    "    return Command(\n",
    "        goto=\"researcher\",\n",
    "        update={\"researcher_messages\": tool_outputs}\n",
    "    )\n",
    "\n",
    "async def compress_research(state: ResearcherState, config: RunnableConfig):\n",
    "    \"\"\"Compress and synthesize research findings into a concise, structured summary.\n",
    "    \n",
    "    This function takes all the research findings, tool outputs, and AI messages from\n",
    "    a researcher's work and distills them into a clean, comprehensive summary while\n",
    "    preserving all important information and findings.\n",
    "    \n",
    "    Args:\n",
    "        state: Current researcher state with accumulated research messages\n",
    "        config: Runtime configuration with compression model settings\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing compressed research summary and raw notes\n",
    "    \"\"\"\n",
    "    # Step 1: Configure the compression model\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    synthesizer_model = configurable_model.with_config({\n",
    "        \"model\": configurable.compression_model,\n",
    "        \"max_tokens\": configurable.compression_model_max_tokens,\n",
    "        \"api_key\": get_api_key_for_model(configurable.compression_model, config),\n",
    "        \"tags\": [\"langsmith:nostream\"]\n",
    "    })\n",
    "    \n",
    "    # Step 2: Prepare messages for compression\n",
    "    researcher_messages = state.get(\"researcher_messages\", [])\n",
    "    \n",
    "    # Add instruction to switch from research mode to compression mode\n",
    "    researcher_messages.append(HumanMessage(content=compress_research_simple_human_message))\n",
    "    \n",
    "    # Step 3: Attempt compression with retry logic for token limit issues\n",
    "    synthesis_attempts = 0\n",
    "    max_attempts = 3\n",
    "    \n",
    "    while synthesis_attempts < max_attempts:\n",
    "        try:\n",
    "            # Create system prompt focused on compression task\n",
    "            compression_prompt = compress_research_system_prompt.format(date=get_today_str())\n",
    "            messages = [SystemMessage(content=compression_prompt)] + researcher_messages\n",
    "            \n",
    "            # Execute compression\n",
    "            response = await synthesizer_model.ainvoke(messages)\n",
    "            \n",
    "            # Extract raw notes from all tool and AI messages\n",
    "            raw_notes_content = \"\\n\".join([\n",
    "                str(message.content) \n",
    "                for message in filter_messages(researcher_messages, include_types=[\"tool\", \"ai\"])\n",
    "            ])\n",
    "            \n",
    "            # Return successful compression result\n",
    "            return {\n",
    "                \"compressed_research\": str(response.content),\n",
    "                \"raw_notes\": [raw_notes_content]\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            synthesis_attempts += 1\n",
    "            \n",
    "            # Handle token limit exceeded by removing older messages\n",
    "            if is_token_limit_exceeded(e, configurable.research_model):\n",
    "                researcher_messages = remove_up_to_last_ai_message(researcher_messages)\n",
    "                continue\n",
    "            \n",
    "            # For other errors, continue retrying\n",
    "            continue\n",
    "    \n",
    "    # Step 4: Return error result if all attempts failed\n",
    "    raw_notes_content = \"\\n\".join([\n",
    "        str(message.content) \n",
    "        for message in filter_messages(researcher_messages, include_types=[\"tool\", \"ai\"])\n",
    "    ])\n",
    "    \n",
    "    return {\n",
    "        \"compressed_research\": \"Error synthesizing research report: Maximum retries exceeded\",\n",
    "        \"raw_notes\": [raw_notes_content]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcc233d",
   "metadata": {},
   "source": [
    "### Compiling our Researcher Subagent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f2b4ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/2fvdkyfj0856p6_6sdvv74rw0000gn/T/ipykernel_9088/2966236398.py:3: LangGraphDeprecatedSinceV10: `config_schema` is deprecated and will be removed. Please use `context_schema` instead. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  researcher_builder = StateGraph(\n",
      "/var/folders/jj/2fvdkyfj0856p6_6sdvv74rw0000gn/T/ipykernel_9088/2966236398.py:3: LangGraphDeprecatedSinceV05: `output` is deprecated and will be removed. Please use `output_schema` instead. Deprecated in LangGraph V0.5 to be removed in V2.0.\n",
      "  researcher_builder = StateGraph(\n"
     ]
    }
   ],
   "source": [
    "# Researcher Subgraph Construction\n",
    "# Creates individual researcher workflow for conducting focused research on specific topics\n",
    "researcher_builder = StateGraph(\n",
    "    ResearcherState, \n",
    "    output=ResearcherOutputState, \n",
    "    config_schema=Configuration\n",
    ")\n",
    "\n",
    "# Add researcher nodes for research execution and compression\n",
    "researcher_builder.add_node(\"researcher\", researcher)                 # Main researcher logic\n",
    "researcher_builder.add_node(\"researcher_tools\", researcher_tools)     # Tool execution handler\n",
    "researcher_builder.add_node(\"compress_research\", compress_research)   # Research compression\n",
    "\n",
    "# Define researcher workflow edges\n",
    "researcher_builder.add_edge(START, \"researcher\")           # Entry point to researcher\n",
    "researcher_builder.add_edge(\"compress_research\", END)      # Exit point after compression\n",
    "\n",
    "# Compile researcher subgraph for parallel execution by supervisor\n",
    "researcher_subgraph = researcher_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd29d2a6",
   "metadata": {},
   "source": [
    "## Supervisor Agent\n",
    "\n",
    "We then implement the supervisor agent in our multiagent system. \n",
    "\n",
    "### Nodes\n",
    "\n",
    "The supervisor will utilize a ReAct architecture, consisting of a reasoning step and a tools step. The supervisor's tools will let it reflect on the task and spin up subagents to conduct deep research.\n",
    "\n",
    "First, the reasoning step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ca4b47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def supervisor(state: SupervisorState, config: RunnableConfig) -> Command[Literal[\"supervisor_tools\"]]:\n",
    "    \"\"\"Lead research supervisor that plans research strategy and delegates to researchers.\n",
    "    \n",
    "    The supervisor analyzes the research brief and decides how to break down the research\n",
    "    into manageable tasks. It can use think_tool for strategic planning, ConductResearch\n",
    "    to delegate tasks to sub-researchers, or ResearchComplete when satisfied with findings.\n",
    "    \n",
    "    Args:\n",
    "        state: Current supervisor state with messages and research context\n",
    "        config: Runtime configuration with model settings\n",
    "        \n",
    "    Returns:\n",
    "        Command to proceed to supervisor_tools for tool execution\n",
    "    \"\"\"\n",
    "    # Step 1: Configure the supervisor model with available tools\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    research_model_config = {\n",
    "        \"model\": configurable.research_model,\n",
    "        \"max_tokens\": configurable.research_model_max_tokens,\n",
    "        \"api_key\": get_api_key_for_model(configurable.research_model, config),\n",
    "        \"tags\": [\"langsmith:nostream\"]\n",
    "    }\n",
    "    \n",
    "    # Available tools: research delegation, completion signaling, and strategic thinking\n",
    "    lead_researcher_tools = [ConductResearch, ResearchComplete, think_tool]\n",
    "    \n",
    "    # Configure model with tools, retry logic, and model settings\n",
    "    research_model = (\n",
    "        configurable_model\n",
    "        .bind_tools(lead_researcher_tools)\n",
    "        .with_retry(stop_after_attempt=configurable.max_structured_output_retries)\n",
    "        .with_config(research_model_config)\n",
    "    )\n",
    "    \n",
    "    # Step 2: Generate supervisor response based on current context\n",
    "    supervisor_messages = state.get(\"supervisor_messages\", [])\n",
    "    response = await research_model.ainvoke(supervisor_messages)\n",
    "    \n",
    "    # Step 3: Update state and proceed to tool execution\n",
    "    return Command(\n",
    "        goto=\"supervisor_tools\",\n",
    "        update={\n",
    "            \"supervisor_messages\": [response],\n",
    "            \"research_iterations\": state.get(\"research_iterations\", 0) + 1\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87eb7b1",
   "metadata": {},
   "source": [
    "Then the tools step, which invokes our previously compiled researcher subagent:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea71b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def supervisor_tools(state: SupervisorState, config: RunnableConfig) -> Command[Literal[\"supervisor\", \"__end__\"]]:\n",
    "    \"\"\"Execute tools called by the supervisor, including research delegation and strategic thinking.\n",
    "    \n",
    "    This function handles three types of supervisor tool calls:\n",
    "    1. think_tool - Strategic reflection that continues the conversation\n",
    "    2. ConductResearch - Delegates research tasks to sub-researchers\n",
    "    3. ResearchComplete - Signals completion of research phase\n",
    "    \n",
    "    Args:\n",
    "        state: Current supervisor state with messages and iteration count\n",
    "        config: Runtime configuration with research limits and model settings\n",
    "        \n",
    "    Returns:\n",
    "        Command to either continue supervision loop or end research phase\n",
    "    \"\"\"\n",
    "    # Step 1: Extract current state and check exit conditions\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    supervisor_messages = state.get(\"supervisor_messages\", [])\n",
    "    research_iterations = state.get(\"research_iterations\", 0)\n",
    "    most_recent_message = supervisor_messages[-1]\n",
    "    \n",
    "    # Define exit criteria for research phase\n",
    "    exceeded_allowed_iterations = research_iterations > configurable.max_researcher_iterations\n",
    "    no_tool_calls = not most_recent_message.tool_calls\n",
    "    research_complete_tool_call = any(\n",
    "        tool_call[\"name\"] == \"ResearchComplete\" \n",
    "        for tool_call in most_recent_message.tool_calls\n",
    "    )\n",
    "    \n",
    "    # Exit if any termination condition is met\n",
    "    if exceeded_allowed_iterations or no_tool_calls or research_complete_tool_call:\n",
    "        return Command(\n",
    "            goto=END,\n",
    "            update={\n",
    "                \"notes\": get_notes_from_tool_calls(supervisor_messages),\n",
    "                \"research_brief\": state.get(\"research_brief\", \"\")\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # Step 2: Process all tool calls together (both think_tool and ConductResearch)\n",
    "    all_tool_messages = []\n",
    "    update_payload = {\"supervisor_messages\": []}\n",
    "    \n",
    "    # Handle think_tool calls (strategic reflection)\n",
    "    think_tool_calls = [\n",
    "        tool_call for tool_call in most_recent_message.tool_calls \n",
    "        if tool_call[\"name\"] == \"think_tool\"\n",
    "    ]\n",
    "    \n",
    "    for tool_call in think_tool_calls:\n",
    "        reflection_content = tool_call[\"args\"][\"reflection\"]\n",
    "        all_tool_messages.append(ToolMessage(\n",
    "            content=f\"Reflection recorded: {reflection_content}\",\n",
    "            name=\"think_tool\",\n",
    "            tool_call_id=tool_call[\"id\"]\n",
    "        ))\n",
    "    \n",
    "    # Handle ConductResearch calls (research delegation)\n",
    "    conduct_research_calls = [\n",
    "        tool_call for tool_call in most_recent_message.tool_calls \n",
    "        if tool_call[\"name\"] == \"ConductResearch\"\n",
    "    ]\n",
    "    \n",
    "    if conduct_research_calls:\n",
    "        try:\n",
    "            # Limit concurrent research units to prevent resource exhaustion\n",
    "            allowed_conduct_research_calls = conduct_research_calls[:configurable.max_concurrent_research_units]\n",
    "            overflow_conduct_research_calls = conduct_research_calls[configurable.max_concurrent_research_units:]\n",
    "            \n",
    "            # Execute research tasks in parallel\n",
    "            research_tasks = [\n",
    "                researcher_subgraph.ainvoke({\n",
    "                    \"researcher_messages\": [\n",
    "                        HumanMessage(content=tool_call[\"args\"][\"research_topic\"])\n",
    "                    ],\n",
    "                    \"research_topic\": tool_call[\"args\"][\"research_topic\"]\n",
    "                }, config) \n",
    "                for tool_call in allowed_conduct_research_calls\n",
    "            ]\n",
    "            \n",
    "            tool_results = await asyncio.gather(*research_tasks)\n",
    "            \n",
    "            # Create tool messages with research results\n",
    "            for observation, tool_call in zip(tool_results, allowed_conduct_research_calls):\n",
    "                all_tool_messages.append(ToolMessage(\n",
    "                    content=observation.get(\"compressed_research\", \"Error synthesizing research report: Maximum retries exceeded\"),\n",
    "                    name=tool_call[\"name\"],\n",
    "                    tool_call_id=tool_call[\"id\"]\n",
    "                ))\n",
    "            \n",
    "            # Handle overflow research calls with error messages\n",
    "            for overflow_call in overflow_conduct_research_calls:\n",
    "                all_tool_messages.append(ToolMessage(\n",
    "                    content=f\"Error: Did not run this research as you have already exceeded the maximum number of concurrent research units. Please try again with {configurable.max_concurrent_research_units} or fewer research units.\",\n",
    "                    name=\"ConductResearch\",\n",
    "                    tool_call_id=overflow_call[\"id\"]\n",
    "                ))\n",
    "            \n",
    "            # Aggregate raw notes from all research results\n",
    "            raw_notes_concat = \"\\n\".join([\n",
    "                \"\\n\".join(observation.get(\"raw_notes\", [])) \n",
    "                for observation in tool_results\n",
    "            ])\n",
    "            \n",
    "            if raw_notes_concat:\n",
    "                update_payload[\"raw_notes\"] = [raw_notes_concat]\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Handle research execution errors\n",
    "            if is_token_limit_exceeded(e, configurable.research_model) or True:\n",
    "                # Token limit exceeded or other error - end research phase\n",
    "                return Command(\n",
    "                    goto=END,\n",
    "                    update={\n",
    "                        \"notes\": get_notes_from_tool_calls(supervisor_messages),\n",
    "                        \"research_brief\": state.get(\"research_brief\", \"\")\n",
    "                    }\n",
    "                )\n",
    "    \n",
    "    # Step 3: Return command with all tool results\n",
    "    update_payload[\"supervisor_messages\"] = all_tool_messages\n",
    "    return Command(\n",
    "        goto=\"supervisor\",\n",
    "        update=update_payload\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbd3c66",
   "metadata": {},
   "source": [
    "### Compiling the Supervisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d1fbfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/2fvdkyfj0856p6_6sdvv74rw0000gn/T/ipykernel_9088/1547228164.py:3: LangGraphDeprecatedSinceV10: `config_schema` is deprecated and will be removed. Please use `context_schema` instead. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  supervisor_builder = StateGraph(SupervisorState, config_schema=Configuration)\n"
     ]
    }
   ],
   "source": [
    "# Supervisor Subgraph Construction\n",
    "# Creates the supervisor workflow that manages research delegation and coordination\n",
    "supervisor_builder = StateGraph(SupervisorState, config_schema=Configuration)\n",
    "\n",
    "# Add supervisor nodes for research management\n",
    "supervisor_builder.add_node(\"supervisor\", supervisor)           # Main supervisor logic\n",
    "supervisor_builder.add_node(\"supervisor_tools\", supervisor_tools)  # Tool execution handler\n",
    "\n",
    "# Define supervisor workflow edges\n",
    "supervisor_builder.add_edge(START, \"supervisor\")  # Entry point to supervisor\n",
    "\n",
    "# Compile supervisor subgraph for use in main workflow\n",
    "supervisor_subgraph = supervisor_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c20121",
   "metadata": {},
   "source": [
    "## Post-Research Workflow\n",
    "\n",
    "We'll cap things off by implementing workflow nodes to summarize our compiled research\n",
    "\n",
    "### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "189c578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def final_report_generation(state: AgentState, config: RunnableConfig):\n",
    "    \"\"\"Generate the final comprehensive research report with retry logic for token limits.\n",
    "    \n",
    "    This function takes all collected research findings and synthesizes them into a \n",
    "    well-structured, comprehensive final report using the configured report generation model.\n",
    "    \n",
    "    Args:\n",
    "        state: Agent state containing research findings and context\n",
    "        config: Runtime configuration with model settings and API keys\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing the final report and cleared state\n",
    "    \"\"\"\n",
    "    # Step 1: Extract research findings and prepare state cleanup\n",
    "    notes = state.get(\"notes\", [])\n",
    "    cleared_state = {\"notes\": {\"type\": \"override\", \"value\": []}}\n",
    "    findings = \"\\n\".join(notes)\n",
    "    \n",
    "    # Step 2: Configure the final report generation model\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    writer_model_config = {\n",
    "        \"model\": configurable.final_report_model,\n",
    "        \"max_tokens\": configurable.final_report_model_max_tokens,\n",
    "        \"api_key\": get_api_key_for_model(configurable.final_report_model, config),\n",
    "        \"tags\": [\"langsmith:nostream\"]\n",
    "    }\n",
    "    \n",
    "    # Step 3: Attempt report generation with token limit retry logic\n",
    "    max_retries = 3\n",
    "    current_retry = 0\n",
    "    findings_token_limit = None\n",
    "    \n",
    "    while current_retry <= max_retries:\n",
    "        try:\n",
    "            # Create comprehensive prompt with all research context\n",
    "            final_report_prompt = final_report_generation_prompt.format(\n",
    "                research_brief=state.get(\"research_brief\", \"\"),\n",
    "                messages=get_buffer_string(state.get(\"messages\", [])),\n",
    "                findings=findings,\n",
    "                date=get_today_str()\n",
    "            )\n",
    "            \n",
    "            # Generate the final report\n",
    "            final_report = await configurable_model.with_config(writer_model_config).ainvoke([\n",
    "                HumanMessage(content=final_report_prompt)\n",
    "            ])\n",
    "            \n",
    "            # Return successful report generation\n",
    "            return {\n",
    "                \"final_report\": final_report.content, \n",
    "                \"messages\": [final_report],\n",
    "                **cleared_state\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Handle token limit exceeded errors with progressive truncation\n",
    "            if is_token_limit_exceeded(e, configurable.final_report_model):\n",
    "                current_retry += 1\n",
    "                \n",
    "                if current_retry == 1:\n",
    "                    # First retry: determine initial truncation limit\n",
    "                    model_token_limit = get_model_token_limit(configurable.final_report_model)\n",
    "                    if not model_token_limit:\n",
    "                        return {\n",
    "                            \"final_report\": f\"Error generating final report: Token limit exceeded, however, we could not determine the model's maximum context length. Please update the model map in deep_researcher/utils.py with this information. {e}\",\n",
    "                            \"messages\": [AIMessage(content=\"Report generation failed due to token limits\")],\n",
    "                            **cleared_state\n",
    "                        }\n",
    "                    # Use 4x token limit as character approximation for truncation\n",
    "                    findings_token_limit = model_token_limit * 4\n",
    "                else:\n",
    "                    # Subsequent retries: reduce by 10% each time\n",
    "                    findings_token_limit = int(findings_token_limit * 0.9)\n",
    "                \n",
    "                # Truncate findings and retry\n",
    "                findings = findings[:findings_token_limit]\n",
    "                continue\n",
    "            else:\n",
    "                # Non-token-limit error: return error immediately\n",
    "                return {\n",
    "                    \"final_report\": f\"Error generating final report: {e}\",\n",
    "                    \"messages\": [AIMessage(content=\"Report generation failed due to an error\")],\n",
    "                    **cleared_state\n",
    "                }\n",
    "    \n",
    "    # Step 4: Return failure result if all retries exhausted\n",
    "    return {\n",
    "        \"final_report\": \"Error generating final report: Maximum retries exceeded\",\n",
    "        \"messages\": [AIMessage(content=\"Report generation failed after maximum retries\")],\n",
    "        **cleared_state\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9faed6",
   "metadata": {},
   "source": [
    "## Final Researcher Graph\n",
    "\n",
    "We'll now bring together all of our components: the Pre-Research nodes, the Supervisor Agent, the Researcher Subagent, and the Post-Research nodes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9573ec27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/2fvdkyfj0856p6_6sdvv74rw0000gn/T/ipykernel_9088/2163481348.py:3: LangGraphDeprecatedSinceV10: `config_schema` is deprecated and will be removed. Please use `context_schema` instead. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  deep_researcher_builder = StateGraph(\n",
      "/var/folders/jj/2fvdkyfj0856p6_6sdvv74rw0000gn/T/ipykernel_9088/2163481348.py:3: LangGraphDeprecatedSinceV05: `input` is deprecated and will be removed. Please use `input_schema` instead. Deprecated in LangGraph V0.5 to be removed in V2.0.\n",
      "  deep_researcher_builder = StateGraph(\n"
     ]
    }
   ],
   "source": [
    "# Main Deep Researcher Graph Construction\n",
    "# Creates the complete deep research workflow from user input to final report\n",
    "deep_researcher_builder = StateGraph(\n",
    "    AgentState, \n",
    "    input=AgentInputState, \n",
    "    config_schema=Configuration\n",
    ")\n",
    "\n",
    "# Add main workflow nodes for the complete research process\n",
    "deep_researcher_builder.add_node(\"clarify_with_user\", clarify_with_user)           # User clarification phase\n",
    "deep_researcher_builder.add_node(\"write_research_brief\", write_research_brief)     # Research planning phase\n",
    "deep_researcher_builder.add_node(\"research_supervisor\", supervisor_subgraph)       # Research execution phase\n",
    "deep_researcher_builder.add_node(\"final_report_generation\", final_report_generation)  # Report generation phase\n",
    "\n",
    "# Define main workflow edges for sequential execution\n",
    "deep_researcher_builder.add_edge(START, \"clarify_with_user\")                       # Entry point\n",
    "deep_researcher_builder.add_edge(\"research_supervisor\", \"final_report_generation\") # Research to report\n",
    "deep_researcher_builder.add_edge(\"final_report_generation\", END)                   # Final exit point\n",
    "\n",
    "# Compile the complete deep researcher workflow\n",
    "# Note - subgraphs inherit checkpointer from parent graph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "checkpointer = MemorySaver()\n",
    "graph = deep_researcher_builder.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddf9451",
   "metadata": {},
   "source": [
    "### Running our Researcher\n",
    "\n",
    "We can run our agent using the code below. NOTE that it does take several minutes to run, and consumes a large amount of tokens. \n",
    "\n",
    "As such, we've also included a sample trace of a successful run.\n",
    "\n",
    "**Public Trace** Trace: https://smith.langchain.com/public/b1258f58-2cfe-4ff8-bed1-e62e5264af3b/r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8af4c9",
   "metadata": {},
   "source": [
    "We'll first set up our configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4d2a6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "max_structured_output_retries = 3\n",
    "allow_clarification = False\n",
    "max_concurrent_research_units = 10\n",
    "search_api = \"tavily\" # NOTE: We use Tavily to stay consistent\n",
    "max_researcher_iterations = 6\n",
    "max_react_tool_calls = 10\n",
    "summarization_model = \"openai:gpt-4.1-mini\"\n",
    "summarization_model_max_tokens = 8192\n",
    "research_model = \"openai:gpt-4.1-mini\" # \"anthropic:claude-sonnet-4-20250514\"\n",
    "research_model_max_tokens = 10000\n",
    "compression_model = \"openai:gpt-4.1-mini\"\n",
    "compression_model_max_tokens = 10000\n",
    "final_report_model = \"openai:gpt-4.1-mini\"\n",
    "final_report_model_max_tokens = 10000\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": str(uuid.uuid4()),\n",
    "        \"max_structured_output_retries\": max_structured_output_retries,\n",
    "        \"allow_clarification\": allow_clarification,\n",
    "        \"max_concurrent_research_units\": max_concurrent_research_units,\n",
    "        \"search_api\": search_api,\n",
    "        \"max_researcher_iterations\": max_researcher_iterations,\n",
    "        \"max_react_tool_calls\": max_react_tool_calls,\n",
    "        \"summarization_model\": summarization_model,\n",
    "        \"summarization_model_max_tokens\": summarization_model_max_tokens,\n",
    "        \"research_model\": research_model,\n",
    "        \"research_model_max_tokens\": research_model_max_tokens,\n",
    "        \"compression_model\": compression_model,\n",
    "        \"compression_model_max_tokens\": compression_model_max_tokens,\n",
    "        \"final_report_model\": final_report_model,\n",
    "        \"final_report_model_max_tokens\": final_report_model_max_tokens\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694dfa09",
   "metadata": {},
   "source": [
    "Then we can run our researcher!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f9a0aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is model context protocol?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "# Understanding the Model Context Protocol (MCP) in Computer Science, AI, and Machine Learning\n",
      "\n",
      "## Introduction\n",
      "\n",
      "The term **Model Context Protocol (MCP)** refers to a recently developed open standard in the field of artificial intelligence (AI) and machine learning (ML), designed to facilitate seamless, secure, and standardized communication between AI models—particularly large language models (LLMs)—and external tools, data sources, or services. Its goal is to enable AI systems to operate beyond isolated model parameters by accessing live, dynamic context and capabilities in a uniform way.\n",
      "\n",
      "This report provides a comprehensive examination of MCP’s definition, functionality, practical applications, significance within AI/ML, and associated standards and frameworks. The explanation draws from authoritative sources including official MCP documentation, leading AI research organizations, and technology standards bodies.\n",
      "\n",
      "---\n",
      "\n",
      "## Definition and Core Function of Model Context Protocol\n",
      "\n",
      "### What is MCP?\n",
      "\n",
      "The **Model Context Protocol (MCP)** is an **open-source, open standard communication protocol** introduced primarily by Anthropic in late 2024. MCP addresses a fundamental integration challenge in AI: how to connect AI models efficiently with diverse external environments such as business tools, data repositories, APIs, and legacy systems without building individual custom interfaces for each combination of model and tool.\n",
      "\n",
      "MCP is often metaphorically referred to as a “**universal USB-C port for AI applications**,” highlighting its role as a generic, plug-and-play protocol enabling models to access external context and functionality securely and reliably [1][3][4].\n",
      "\n",
      "### How MCP Works\n",
      "\n",
      "MCP uses standardized messaging based on **JSON-RPC 2.0**, a lightweight remote procedure call protocol well-suited for communication between AI applications and external services. Communication can occur synchronously or asynchronously, and MCP supports transport over:\n",
      "\n",
      "- Local connections via Standard Input/Output (STDIO)\n",
      "- Remote connections using HTTP with Server-Sent Events (SSE)\n",
      "\n",
      "Its architecture typically includes three main components:\n",
      "\n",
      "- **MCP Host / Orchestrator:** The environment managing AI model instances and coordinating MCP communications.\n",
      "- **MCP Client:** Converts user or application requests into standardized MCP messages and manages protocol-level communication.\n",
      "- **MCP Server(s):** Interface adapters connecting MCP messages to actual external tools or data sources.\n",
      "\n",
      "Communication begins with a **handshake phase**, where the client and server announce capabilities and permissions, ensuring compatibility and security before exchanging operational requests. This exchange allows AI models to ask for data or invoke tools dynamically, reducing the need to embed vast amounts of static context into the model itself, which in turn lowers hallucination risks and improves output accuracy [2][5][6].\n",
      "\n",
      "### Security Model\n",
      "\n",
      "Security is central to MCP’s design. The protocol incorporates:\n",
      "\n",
      "- **OAuth 2.0** for robust authentication and authorization flows.\n",
      "- **Encrypted communication channels** to protect data integrity and confidentiality.\n",
      "- **Explicit user consent and fine-grained permission controls** following the principle of least privilege to limit AI access only to approved tools and data.\n",
      "- Safeguards against vulnerabilities like **prompt injection** or unauthorized execution of actions.\n",
      "\n",
      "These mechanisms enable enterprise-grade security suitable for sensitive or regulated domains [4][5][10].\n",
      "\n",
      "---\n",
      "\n",
      "## Practical Applications of Model Context Protocol\n",
      "\n",
      "MCP serves as a backbone for enabling AI models to function as **context-aware agents** capable of complex, real-world tasks that require fetching live data, interacting with multiple platforms, and automating workflows. Key practical applications include:\n",
      "\n",
      "- **AI Assistants and Chatbots:** Allow LLMs like Anthropic’s Claude, OpenAI’s ChatGPT, or Google’s Gemini to access live project files, databases, calendar events, or communication platforms (e.g., Slack, Google Drive, GitHub), enhancing their usefulness and factual precision.\n",
      "- **Natural Language Database Interfaces:** AI querying of SQL/NoSQL databases via conversational interfaces for business intelligence and analytics.\n",
      "- **AI-Powered Software Development:** Real-time integration with source code repositories, issue trackers, and deployment pipelines enabling AI pair programming and continuous integration automation.\n",
      "- **Enterprise Workflow Automation:** Automating multi-step processes such as document review, customer support ticket triage, research synthesis, financial analysis, or CRM updates.\n",
      "\n",
      "Sectors benefiting from MCP-enabled AI workflows include legal, pharmaceuticals, architecture, construction, and other industries demanding both high accuracy and secure access to internal data [1][3][5][6][9][11].\n",
      "\n",
      "---\n",
      "\n",
      "## Importance of Model Context Protocol in AI and Machine Learning\n",
      "\n",
      "### Solving Integration Complexity: The N×M Problem\n",
      "\n",
      "Before MCP, integrating multiple AI models (N) with various tools or data sources (M) required custom connectors, resulting in an N×M combinatorial explosion of integration effort. MCP standardizes the communication layer, dramatically reducing development overhead and simplifying AI application ecosystems.\n",
      "\n",
      "### Fostering Interoperability and Ecosystem Growth\n",
      "\n",
      "By providing a universal communication standard, MCP enables interoperability between different AI models and a broad range of services, fostering an ecosystem where integrations are reusable, extensible, and community-driven.\n",
      "\n",
      "### Enhancing AI Capabilities and Reliability\n",
      "\n",
      "MCP allows models to access dynamic, up-to-date context securely, which:\n",
      "\n",
      "- Improves the factual grounding of AI outputs by providing real-time information.\n",
      "- Enables **retrieval-augmented generation (RAG)** where models combine generative capabilities with live data retrieval.\n",
      "- Supports complex multi-agent orchestrations where multiple AI tools cooperate seamlessly.\n",
      "\n",
      "### Adoption and Industry Impact\n",
      "\n",
      "Since its introduction, MCP has gained rapid acceptance by major AI players including Anthropic (Claude), OpenAI (ChatGPT), Google DeepMind, Microsoft, and Zapier, appearing as foundational infrastructure in AI platforms and tools. Its adoption underscores MCP’s critical role in evolving LLMs from isolated models into versatile, contextually aware AI agents [2][4][8][9][11].\n",
      "\n",
      "---\n",
      "\n",
      "## Standards, Frameworks, and Ecosystem of MCP\n",
      "\n",
      "### Official Specifications and Protocol Standards\n",
      "\n",
      "- MCP uses **JSON-RPC 2.0** to define message formats and procedures between AI systems and external services.\n",
      "- The protocol specification is publicly available and under continuous enhancement, emphasizing security, extensibility, and usability [7].\n",
      "\n",
      "### SDKs and Reference Implementations\n",
      "\n",
      "MCP is supported by official SDKs and client/server reference implementations in multiple programming languages including:\n",
      "\n",
      "- Python\n",
      "- TypeScript / JavaScript\n",
      "- C#\n",
      "- Java\n",
      "\n",
      "These enable developers and organizations to build MCP-compliant tools efficiently [4][6][8][10].\n",
      "\n",
      "### MCP Servers and Integrations\n",
      "\n",
      "There is a growing ecosystem of MCP servers that act as adapters to various domains and services, including:\n",
      "\n",
      "- Version control and code hosting (e.g., Git)\n",
      "- File systems\n",
      "- Cloud APIs and HTTP fetch\n",
      "- Business platforms (Slack, Stripe, HubSpot, Supabase)\n",
      "- Community-contributed servers for additional platforms like Discord and Docker\n",
      "\n",
      "### Security Alignment and Enforcement\n",
      "\n",
      "The protocol’s security model integrates OAuth 2.0 standards for authentication and authorization, supporting enterprise features such as dynamic client registration and token lifecycle management. Future protocol enhancements are planned to refine permission scoping, client identity metadata, and secure elicitation modes, balancing greater automation with user trust and safety [6][10][11].\n",
      "\n",
      "### Relationship to AI Agent Frameworks\n",
      "\n",
      "While MCP is not itself an agent or orchestration framework, it complements higher-level tools like LangChain and BeeAI by providing a consistent standard for AI-tool communication. These frameworks leverage MCP to build complex multiagent workflows with reliable, standardized tool access [2].\n",
      "\n",
      "---\n",
      "\n",
      "## Summary\n",
      "\n",
      "The **Model Context Protocol (MCP)** is a groundbreaking open standard that enables AI models—especially large language models—to connect securely and consistently with external data sources, tools, and workflows. By providing a universal communication protocol based on JSON-RPC 2.0 and secured through OAuth 2.0, MCP solves crucial integration issues in AI development, allowing models to consume live context and execute diverse, autonomous workflows.\n",
      "\n",
      "Its practical applications span AI assistants, natural language interfaces to databases, software development, and enterprise automation across multiple industries. MCP is vital in transforming static, isolated AI models into dynamic, context-aware agents, thereby enhancing model reliability, reducing hallucinations, and fostering an interoperable AI ecosystem.\n",
      "\n",
      "With growing adoption by leading AI companies and a rich ecosystem of SDKs and servers, MCP represents a foundational advancement in AI infrastructure, promising ongoing enhancements and broad impact on AI-driven applications [1][2][3][4][5][6][7][8][9][10][11].\n",
      "\n",
      "---\n",
      "\n",
      "### Sources\n",
      "\n",
      "[1] Model Context Protocol — https://modelcontextprotocol.io/  \n",
      "[2] What is Model Context Protocol (MCP)? - IBM — https://www.ibm.com/think/topics/model-context-protocol  \n",
      "[3] Introducing the Model Context Protocol - Anthropic — https://www.anthropic.com/news/model-context-protocol  \n",
      "[4] Model Context Protocol - Wikipedia — https://en.wikipedia.org/wiki/Model_Context_Protocol  \n",
      "[5] What is Model Context Protocol (MCP)? - Red Hat — https://www.redhat.com/en/topics/ai/what-is-model-context-protocol-mcp  \n",
      "[6] What Is the Model Context Protocol (MCP) and How It Works — https://www.descope.com/learn/post/mcp  \n",
      "[7] Specification - Model Context Protocol — https://modelcontextprotocol.io/specification/2025-06-18  \n",
      "[8] Model Context Protocol: Simplifying AI Tool Integration - Aerospike — https://aerospike.com/blog/model-context-protocol/  \n",
      "[9] Why Your Company Should Know About Model Context Protocol — https://www.nasuni.com/blog/why-your-company-should-know-about-model-context-protocol/  \n",
      "[10] Model Context Protocol (MCP): A comprehensive introduction — https://stytch.com/blog/model-context-protocol-introduction/  \n",
      "[11] Model Context Protocol: A New Standard for Connecting AI to the Real World — https://www.synechron.com/en-us/insight/model-context-protocol-new-standard-connecting-ai-real-world\n"
     ]
    }
   ],
   "source": [
    "msg = [{\"role\": \"user\", \"content\": \"What is model context protocol?\"}]\n",
    "\n",
    "response = await graph.ainvoke({\"messages\": msg}, config)\n",
    "\n",
    "for message in response[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
